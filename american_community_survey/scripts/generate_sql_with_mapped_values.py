import pandas as pd
import sys
import os
import json

def generate_materialized_name(folder_name, csv_name, state_lookup, national_lookup):
    type_char = folder_name.split("_")[1][0].lower()
    folder_code = folder_name.split("_")[1][1:].upper()
    human_readable_name = "individual_people" if type_char == "p" else "housing_units"

    if folder_code == "US":
        csv_code = csv_name.split("_")[1][1:].upper()
        name = national_lookup.get(csv_code, "Unknown national code")
    elif len(folder_code) == 2:
        name = state_lookup.get(folder_code, "Unknown state code")
    else:
        raise ValueError(f"Invalid code: {folder_code}")
    
    return f"{human_readable_name}_{name.replace(' ', '_')}".lower()


def clean_enum_value(value):
    value = value.replace("'", "")
    value = value.replace("N/A", "Not applicable")
    value = value.replace("/", " or ")
    value = value.replace("(", "- ")
    value = value.replace(")", "")
    return value


if len(sys.argv) < 3:
    print("Usage: python script.py <parquet_database_path> <PUMS_data_dictionary_path>")
    sys.exit(1)

parquet_database_path, data_dictionary_path = sys.argv[1:3]

with open(data_dictionary_path, "r") as json_file:
    data_dict = json.load(json_file)

state_lookup = {code: name for name, code in [x.split("/") for x in data_dict["ST"]["Values"].values()]}
national_lookup = {"USA": "United States first tranche", "USB": "United States second tranche"}

df_csv_paths = pd.read_parquet(parquet_database_path)
models_dir = "models/public_use_microdata_sample/generated/mapped_values"
os.makedirs(models_dir, exist_ok=True)

for csv_path in df_csv_paths["csv_path"]:
    folder_name = os.path.basename(os.path.dirname(csv_path))
    csv_name = os.path.basename(csv_path).split(".")[0]
    materialized_name = generate_materialized_name(folder_name, csv_name, state_lookup, national_lookup)

    df_headers = pd.read_csv(csv_path, nrows=0)
    column_types = {column: 'VARCHAR' for column in df_headers.columns}
    columns = ', '.join([f"'{col}': '{typ}'" for col, typ in column_types.items()])

    sql_select_parts = ["SELECT"]
    enum_creation_statements = []

    for header in df_headers.columns:
        if header in data_dict and "Values" in data_dict[header]:
            # Mapping codes to labels using CASE statement
            if any(['Integer weight' in value for value in data_dict[header]["Values"].values()]):
                print(f"Selecting unmapped {header} because it contains an 'Integer weight' value")
                sql_select_parts.append(f"    {header},")
            else:
                # value_mapping = ' '.join([f"WHEN '{clean_enum_value(code)}' THEN '{clean_enum_value(label)}'" for code, label in data_dict[header]["Values"].items()])
                # mapped_column = f"CASE {header} {value_mapping} END AS {header}"
                # sql_select_parts.append(f"    {mapped_column},")
                            # Improved mapping with pretty printing
                
                col_info = data_dict.get(header, {"Description": header})
                description = col_info["Description"]
                value_mapping = "\n\t\t".join([
                    f"WHEN '{clean_enum_value(code)}' THEN '{clean_enum_value(label)}'"
                    for code, label in data_dict[header]["Values"].items()
                ])
                # mapped_column = f"""CASE {header}\n\t\t{value_mapping}\n\tEND AS \"{description}\""""
                mapped_column = f"""CASE {header}\n\t\t{value_mapping}\n\tEND AS {header}"""
                sql_select_parts.append(f"\t{mapped_column},")
        elif header in data_dict:
            # Direct mapping for columns without "Values"
            description = data_dict[header]["Description"].replace("'", "''")
            # sql_select_parts.append(f'    {header} AS "{description}",')
            sql_select_parts.append(f'    {header},')
        else:
            sql_select_parts.append(f"    {header},")
    
    sql_select_parts[-1] = sql_select_parts[-1].rstrip(',')
    sql_select_statement = "\n".join(sql_select_parts)
    newline = "\n"
    sql_content = f"""-- SQL transformation for {csv_name} generated by {os.path.basename(__file__)}
{newline.join(enum_creation_statements)}

{{{{ config(materialized='external', location=var('output_path') + '/{materialized_name}.parquet') }}}}
{sql_select_statement}
FROM read_csv('{csv_path}', 
              parallel=False,
              all_varchar=True,
              auto_detect=True)"""

    sql_file_path = os.path.join(models_dir, f"{materialized_name}_mapped.sql")
    with open(sql_file_path, "w") as sql_file:
        sql_file.write(sql_content)
